{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514a161a",
   "metadata": {},
   "source": [
    "# Generalized FPI Data Processing Notebook\n",
    "\n",
    "This notebook provides a set of functions to load, clean, filter, and save FPI data from `.hdf5` files. The main logic is encapsulated in the `process_fpi_data` function, allowing it to be easily used for different datasets (like Lowell, Urbana, etc.) by simply changing the input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764395e",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7572a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from datetime import datetime, timezone\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be054be",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "These are general-purpose functions used in the main processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae80ef84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamps_to_utc(timestamps):\n",
    "    \"\"\"\n",
    "    Convert an array of UNIX timestamps to UTC datetime objects.\n",
    "    \"\"\"\n",
    "    return [datetime.utcfromtimestamp(ts).replace(tzinfo=timezone.utc) for ts in timestamps]\n",
    "\n",
    "def remove_outliers_by_sigma(df, sigma=3, columns=None):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame if a value is an outlier based on the sigma rule, computed per hourly bin.\n",
    "\n",
    "    An outlier is a value outside the range [mean - sigma * std, mean + sigma * std] for each bin.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame with a 'timestamps' column.\n",
    "    - sigma (float): Number of standard deviations for the outlier threshold.\n",
    "    - columns (list): Column names to check for outliers. Checks all numeric columns if None.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A new DataFrame with outlier rows removed.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    columns_to_check = columns if columns is not None else df_out.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    df_out['datetime'] = pd.to_datetime(df_out['timestamps'], unit='s')\n",
    "    \n",
    "    # Binning by hour of the day. You can modify this for other frequencies.\n",
    "    # For example, use `df_out['datetime'].dt.date` for daily bins.\n",
    "    df_out['bin'] = df_out['datetime'].dt.hour\n",
    "\n",
    "    grouped = df_out.groupby('bin')\n",
    "    filtered_groups = []\n",
    "    \n",
    "    for bin_id, group in grouped:\n",
    "        if group.empty:\n",
    "            continue\n",
    "        \n",
    "        group_filtered = group.copy()\n",
    "        for col in columns_to_check:\n",
    "            if col in group_filtered.columns:\n",
    "                mean_val = group_filtered[col].mean()\n",
    "                std_val = group_filtered[col].std()\n",
    "\n",
    "                if pd.isna(std_val) or std_val == 0:\n",
    "                    continue\n",
    "\n",
    "                lower_bound = mean_val - sigma * std_val\n",
    "                upper_bound = mean_val + sigma * std_val\n",
    "                group_filtered = group_filtered[(group_filtered[col] >= lower_bound) & (group_filtered[col] <= upper_bound)]\n",
    "        \n",
    "        filtered_groups.append(group_filtered)\n",
    "    \n",
    "    result = pd.concat(filtered_groups) if filtered_groups else pd.DataFrame(columns=df.columns)\n",
    "    result = result.drop(columns=['datetime', 'bin'], errors='ignore')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24734ecb",
   "metadata": {},
   "source": [
    "## 3. Main Data Processing Function\n",
    "\n",
    "This function orchestrates the entire workflow: loading, filtering, visualizing, and saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb22f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_fpi_data(directory_path, site_name, output_base_path, oh_lower = 0, oh_upper = 200, sigma=3):\n",
    "    \"\"\"\n",
    "    Loads, processes, and saves FPI data from a specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory_path (str): Path to the directory containing .hdf5 files.\n",
    "    - site_name (str): Name of the site (e.g., 'Lowell') for titles and filenames.\n",
    "    - output_base_path (str): Base directory to save processed data.\n",
    "    - oh_lower (float): Lower bound for the relative emission brightness filter.\n",
    "    - oh_upper (float): Upper bound for the relative emission brightness filter.\n",
    "    - sigma (float): Sigma value for outlier clipping.\n",
    "    \"\"\"\n",
    "    # --- 1. Load Data ---\n",
    "    file_list = glob.glob(os.path.join(directory_path, '*.hdf5'))\n",
    "    if not file_list:\n",
    "        print(f\"Error: No .hdf5 files found in '{directory_path}'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(file_list)} files to process for {site_name}.\")\n",
    "    list_of_dfs = []\n",
    "    required_keys = ['timestamps', 'cloudind', 'temp_err', 'wind_err', 'tn', 'dtn', 'vnu', 'dvnu', 'rlel', 'azm', 'elm']\n",
    "\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                if all(key in f for key in required_keys):\n",
    "                    list_of_dfs.append(pd.DataFrame({key: f[key][:] for key in required_keys}))\n",
    "                else:\n",
    "                    print(f\"Skipping file {os.path.basename(file_path)}: Missing required datasets.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process file {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    if not list_of_dfs:\n",
    "        print(\"No valid data loaded.\")\n",
    "        return\n",
    "\n",
    "    df = pd.concat(list_of_dfs, ignore_index=True)\n",
    "    df['rle_linear'] = 10**df['rlel']  # Convert log brightness to linear\n",
    "    print(f\"\\nSuccessfully loaded data from {len(list_of_dfs)} files. Total rows: {len(df)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # --- 2. Brightness Threshold Analysis ---\n",
    "    print(\"Analyzing zenith data to help determine OH brightness threshold...\")\n",
    "    zenith_df = df[df['elm'] > 85].copy()\n",
    "\n",
    "    if not zenith_df.empty:\n",
    "        max_rle = zenith_df['rle_linear'].max()\n",
    "        bins = np.arange(0, max_rle + 0.1, 0.1) # change the bin sizes as needed np.arange(0, max_rle + 0.5, 0.5) for example\n",
    "        zenith_df['brightness_bin'] = pd.cut(zenith_df['rle_linear'], bins=bins, right=False)\n",
    "        wind_deviation = zenith_df.groupby('brightness_bin')['vnu'].apply(lambda x: np.abs(x).mean()).reset_index()\n",
    "        wind_deviation['bin_mid'] = wind_deviation['brightness_bin'].apply(lambda b: b.mid)\n",
    "\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(wind_deviation['bin_mid'], wind_deviation['vnu'], marker='o', linestyle='-', label='Mean Wind Deviation')\n",
    "        ax.axhline(20, color='crimson', linestyle='--', label='20 m/s Threshold')\n",
    "        ax.set_title(f'Wind Deviation vs. Brightness ({site_name} Zenith Data)')\n",
    "        ax.set_xlabel('Linear Relative Emission (Brightness)')\n",
    "        ax.set_ylabel('Mean Absolute Wind Speed |vnu| (m/s)')\n",
    "        #ax.set_xlim(left=0, right=50)\n",
    "        #ax.set_ylim(bottom=0)\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No zenith data found; skipping OH threshold plot.\")\n",
    "\n",
    "    # --- 3. Quality Control Filtering ---\n",
    "    print(\"\\nApplying quality control filters...\")\n",
    "    original_rows = len(df)\n",
    "    mask = (\n",
    "        (df['cloudind'] < -25) &\n",
    "        (df['temp_err'].isin([0, 1])) &\n",
    "        (df['wind_err'].isin([0, 1])) &\n",
    "        (df['dvnu'] < 100) &\n",
    "        (df['tn'] > 150) &\n",
    "        (df['dtn'] < 100) &\n",
    "        (df['rle_linear'] >= oh_lower) & (df['rle_linear'] <= oh_upper)\n",
    "    )\n",
    "    clean_df = df[mask].copy()\n",
    "    print(f\"\\nFiltering complete. Kept {len(clean_df)} of {original_rows} samples.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # --- 4. Sigma Outlier Clipping ---\n",
    "    print(f\"Applying {sigma}-sigma outlier clipping...\")\n",
    "    clipped_df = remove_outliers_by_sigma(clean_df, columns=['tn', 'vnu'], sigma=sigma)\n",
    "    \n",
    "    print(f\"\\nClipping complete. Original: {len(clean_df)}, After clipping: {len(clipped_df)}, Removed: {len(clean_df) - len(clipped_df)}\")\n",
    "    \n",
    "    # --- 5. Visualization of Final Data ---\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    axs[0].hist(clipped_df['rle_linear'], bins=50, color='royalblue', edgecolor='black')\n",
    "    axs[0].set_title(f'Distribution of rle_linear in Final Data ({site_name})')\n",
    "    axs[0].set_xlabel('Relative Emission (rle_linear)')\n",
    "    axs[0].set_ylabel('Count')\n",
    "\n",
    "    axs[1].scatter(clipped_df['rle_linear'], clipped_df['vnu'], alpha=0.3, color='darkorange', s=5)\n",
    "    axs[1].set_title(f'Wind Velocity vs. rle_linear ({site_name})')\n",
    "    axs[1].set_xlabel('Relative Emission (rle_linear)')\n",
    "    axs[1].set_ylabel('Wind Velocity (vnu) (m/s)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 6. Save Data ---\n",
    "    output_directory = os.path.join(output_base_path, site_name)\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    output_file = os.path.join(output_directory, f'{site_name}_processed.nc')\n",
    "    ds = clipped_df.to_xarray()\n",
    "    ds.to_netcdf(output_file)\n",
    "    print(f\"\\nFinal processed data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da13c1cc",
   "metadata": {},
   "source": [
    "## 4. Example Usage\n",
    "\n",
    "To run the processing for a new site, modify the variables in the cell below and run it. \n",
    "\n",
    "1.  **`SITE_DATA_DIR`**: Set this to the folder containing your `.hdf5` files.\n",
    "2.  **`SITE_NAME`**: A descriptive name for your location (e.g., 'Urbana').\n",
    "3.  **`OUTPUT_DIR`**: The base folder where the processed data will be saved.\n",
    "4.  **`OH_LOWER`/`OH_UPPER`**: Adjust these based on the 'Wind Deviation vs. Brightness' plot generated for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cc76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    SITE_DATA_DIR = '/mnt/madrigal_downloads/Lowell_FPI/' \n",
    "    SITE_NAME = 'Lowell'\n",
    "    OUTPUT_DIR = '/mnt/madrigal_downloads/processedData/'\n",
    "    \n",
    "    # Brightness thresholds - adjust after reviewing the plot\n",
    "    OH_LOWER_BOUND = 0\n",
    "    OH_UPPER_BOUND = 200\n",
    "\n",
    "    # Sigma for outlier clipping\n",
    "    SIGMA_VALUE = 3\n",
    "    \n",
    "    # --- Run Processing ---\n",
    "    process_fpi_data(\n",
    "        directory_path=SITE_DATA_DIR,\n",
    "        site_name=SITE_NAME,\n",
    "        output_base_path=OUTPUT_DIR,\n",
    "        oh_lower=OH_LOWER_BOUND,\n",
    "        oh_upper=OH_UPPER_BOUND,\n",
    "        sigma=SIGMA_VALUE\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
